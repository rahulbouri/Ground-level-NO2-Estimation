{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T07:20:20.303790Z",
     "start_time": "2024-09-15T07:20:20.290717Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from model import AttentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T07:20:21.177630Z",
     "start_time": "2024-09-15T07:20:21.093096Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Train_Cleaned_KNN_Filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NO2Dataset(Dataset):\n",
    "    def __init__(self, df, max_days=15):\n",
    "        self.max_days = max_days\n",
    "        self.data = df.sort_values(by=['LAT', 'LON', 'Date']).reset_index(drop=True)\n",
    "        self.data['Date'] = pd.to_datetime(self.data['Date'])\n",
    "        self.locations = self.data.groupby(['LAT', 'LON']).groups\n",
    "        self.location_keys = list(self.locations.keys())\n",
    "        self.samples = [(loc, idx) for loc in self.location_keys for idx in self.locations[loc]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        location, index = self.samples[idx]\n",
    "        location_data = self.data.loc[self.locations[location]].reset_index(drop=True)\n",
    "\n",
    "        location_data['Date'] = pd.to_datetime(location_data['Date'], errors='coerce')\n",
    "        location_data = location_data.dropna(subset=['Date'])\n",
    "\n",
    "        for i in range(len(location_data)):\n",
    "            current_date = location_data.loc[i, 'Date']\n",
    "            start_date = current_date - pd.DateOffset(days=self.max_days)\n",
    "            end_date = current_date\n",
    "\n",
    "            past_data = location_data[(location_data['Date'] >= start_date) & (location_data['Date'] < end_date)]\n",
    "\n",
    "            if len(past_data) < self.max_days:\n",
    "                num_padding_days = self.max_days - len(past_data)\n",
    "                last_date = past_data['Date'].min() if not past_data.empty else pd.Timestamp.now()\n",
    "\n",
    "                padding_data = pd.DataFrame({\n",
    "                    'Date': [last_date - pd.DateOffset(days=i+1) for i in range(num_padding_days)]\n",
    "                }).reindex(columns=past_data.columns, fill_value=0)\n",
    "\n",
    "                past_data = pd.concat([padding_data, past_data], ignore_index=True)\n",
    "\n",
    "            past_data = past_data.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "            # Extract features\n",
    "            features_tensor = torch.tensor( past_data[['LST', 'AAI', 'CloudFraction', 'Precipitation', 'NO2_strat', 'NO2_total', 'NO2_trop', 'TropopausePressure']].values, dtype=torch.float32)\n",
    "            lat = torch.tensor(past_data[['LAT']].values, dtype=torch.float32)\n",
    "            lon = torch.tensor(past_data[['LON']].values, dtype=torch.float32)\n",
    "\n",
    "            gt = torch.tensor(past_data[['GT_NO2']].values, dtype=torch.float32)\n",
    "\n",
    "            # Return tensors for this sample\n",
    "            return features_tensor, lat, lon, gt\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features_list, lat_list, lon_list, gt_list = zip(*batch)\n",
    "\n",
    "    # Stack tensors to create batch\n",
    "    features_tensor_batch = torch.stack(features_list)\n",
    "    lat_tensor_batch = torch.stack(lat_list)\n",
    "    lon_tensor_batch = torch.stack(lon_list)\n",
    "    gt_batch = torch.stack(gt_list)\n",
    "\n",
    "    return features_tensor_batch, lat_tensor_batch, lon_tensor_batch, gt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T07:23:18.939097Z",
     "start_time": "2024-09-15T07:23:18.732055Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "no2_dataset = NO2Dataset(dataset)\n",
    "train_size = int(0.8 * len(no2_dataset))\n",
    "val_size = len(no2_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(no2_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 15, 8]) torch.Size([8, 15, 1]) torch.Size([8, 15, 1]) torch.Size([8, 15, 1])\n"
     ]
    }
   ],
   "source": [
    "for features, lat, lon, gt in train_loader:\n",
    "    print(features.shape, lat.shape, lon.shape, gt.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Compute Mean Squared Error (MSE)\n",
    "        mse = torch.mean((predictions - targets) ** 2)\n",
    "        # Return the square root of MSE\n",
    "        rms = torch.sqrt(mse)\n",
    "        return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, model, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    model.to(device)  # Move model to the GPU\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, (features_seq, lat, lon, gt) in enumerate(train_loader):\n",
    "        # Move data to GPU\n",
    "        features_seq, lat, lon, gt = features_seq.to(device), lat.to(device), lon.to(device), gt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features_seq, lat, lon)\n",
    "        loss = criterion(outputs.squeeze(), gt)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 99 == 0:\n",
    "            last_loss = running_loss / 100  # Average loss over 100 batches\n",
    "            print(f'Epoch {epoch_index}, Batch {i+1}, Loss: {last_loss}')\n",
    "\n",
    "            # TensorBoard logging\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "def validate_one_epoch(epoch_index, tb_writer, model, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)  # Move model to the GPU\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (features_seq, lat, lon, gt) in enumerate(val_loader):\n",
    "            # Move data to GPU\n",
    "            features_seq, lat, lon, gt = features_seq.to(device), lat.to(device), lon.to(device), gt.to(device)\n",
    "\n",
    "            val_outputs = model(features_seq, lat, lon)\n",
    "            loss = criterion(val_outputs.squeeze(), gt)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and ground truths for RMSE calculation\n",
    "            all_preds.append(val_outputs.squeeze().cpu().numpy())\n",
    "            all_gts.append(gt.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_gts = np.concatenate(all_gts)\n",
    "    val_rmse = np.sqrt(np.mean((all_preds - all_gts) ** 2))\n",
    "\n",
    "    print(f'Epoch {epoch_index}, Validation RMSE: {val_rmse:.4f}')\n",
    "    tb_writer.add_scalar('RMSE/validation', val_rmse, epoch_index)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 146192\n"
     ]
    }
   ],
   "source": [
    "model = AttentionModel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of parameters in the model: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 1, Loss: 0.0031827571988105774\n",
      "Epoch 0, Batch 100, Loss: 0.17591502249240876\n",
      "Epoch 0, Batch 199, Loss: 0.08625438086688518\n",
      "Epoch 0, Batch 298, Loss: 0.05008761234581471\n",
      "Epoch 0, Batch 397, Loss: 0.03302999973297119\n",
      "Epoch 0, Batch 496, Loss: 0.02376102942973375\n",
      "Epoch 0, Batch 595, Loss: 0.01804205322638154\n",
      "Epoch 0, Batch 694, Loss: 0.014225746178999544\n",
      "Epoch 0, Batch 793, Loss: 0.011534760892391204\n",
      "Epoch 0, Batch 892, Loss: 0.009557036804035307\n",
      "Epoch 0, Batch 991, Loss: 0.008055639597587287\n",
      "Epoch 0, Batch 1090, Loss: 0.00688572645187378\n",
      "Epoch 0, Batch 1189, Loss: 0.005954342833720147\n",
      "Epoch 0, Batch 1288, Loss: 0.0051994227292016144\n",
      "Epoch 0, Batch 1387, Loss: 0.004578120252117515\n",
      "Epoch 0, Batch 1486, Loss: 0.004060045680962503\n",
      "Epoch 0, Batch 1585, Loss: 0.0036231012875214217\n",
      "Epoch 0, Batch 1684, Loss: 0.003250891463831067\n",
      "Epoch 0, Batch 1783, Loss: 0.002931013717316091\n",
      "Epoch 0, Batch 1882, Loss: 0.002653953970875591\n",
      "Epoch 0, Batch 1981, Loss: 0.0024122981494292617\n",
      "Epoch 0, Batch 2080, Loss: 0.0022001891233958304\n",
      "Epoch 0, Batch 2179, Loss: 0.002012958051636815\n",
      "Epoch 0, Batch 2278, Loss: 0.0018468308087904006\n",
      "Epoch 0, Batch 2377, Loss: 0.0016987357870675624\n",
      "Epoch 0, Batch 2476, Loss: 0.001566147479461506\n",
      "Epoch 0, Batch 2575, Loss: 0.0014469773031305522\n",
      "Epoch 0, Batch 2674, Loss: 0.0013394795567728578\n",
      "Epoch 0, Batch 2773, Loss: 0.001242188261821866\n",
      "Epoch 0, Batch 2872, Loss: 0.0011538625450339167\n",
      "Epoch 0, Batch 2971, Loss: 0.0010734491457697003\n",
      "Epoch 0, Batch 3070, Loss: 0.0010000450618099421\n",
      "Epoch 0, Batch 3169, Loss: 0.00093287555326242\n",
      "Epoch 0, Batch 3268, Loss: 0.0008712711330736056\n",
      "Epoch 0, Batch 3367, Loss: 0.0008146498730638996\n",
      "Epoch 0, Batch 3466, Loss: 0.0007625060458667576\n",
      "Epoch 0, Batch 3565, Loss: 0.000714396039256826\n",
      "Epoch 0, Batch 3664, Loss: 0.0006699307769304141\n",
      "Epoch 0, Batch 3763, Loss: 0.0006287675275234505\n",
      "Epoch 0, Batch 3862, Loss: 0.0005906028638128191\n",
      "Epoch 0, Batch 3961, Loss: 0.0005551677470793947\n",
      "Epoch 0, Batch 4060, Loss: 0.0005222224351018667\n",
      "Epoch 0, Batch 4159, Loss: 0.0004915542199159973\n",
      "Epoch 0, Batch 4258, Loss: 0.00046297087479615586\n",
      "Epoch 0, Batch 4357, Loss: 0.00043630150175886227\n",
      "Epoch 0, Batch 4456, Loss: 0.0004113913100445643\n",
      "Epoch 0, Batch 4555, Loss: 0.0003881011044722982\n",
      "Epoch 0, Batch 4654, Loss: 0.0003663052618503571\n",
      "Epoch 0, Batch 4753, Loss: 0.00034588966780574993\n",
      "Epoch 0, Batch 4852, Loss: 0.0003267511748708785\n",
      "Epoch 0, Batch 4951, Loss: 0.0003087956420495175\n",
      "Epoch 0, Batch 5050, Loss: 0.00029193745780503377\n",
      "Epoch 0, Batch 5149, Loss: 0.0002760982103063725\n",
      "Epoch 0, Batch 5248, Loss: 0.00026120637165149673\n",
      "Epoch 0, Batch 5347, Loss: 0.00024719668246689255\n",
      "Epoch 0, Batch 5446, Loss: 0.00023400873309583402\n",
      "Epoch 0, Batch 5545, Loss: 0.0002215874323155731\n",
      "Epoch 0, Batch 5644, Loss: 0.00020988201969885268\n",
      "Epoch 0, Batch 5743, Loss: 0.00019884547204128465\n",
      "Epoch 0, Batch 5842, Loss: 0.0001884347321174573\n",
      "Epoch 0, Batch 5941, Loss: 0.00017860972322523595\n",
      "Epoch 0, Batch 6040, Loss: 0.0001693335367599502\n",
      "Epoch 0, Batch 6139, Loss: 0.00016057193919550628\n",
      "Epoch 0, Batch 6238, Loss: 0.0001522930974897463\n",
      "Epoch 0, Batch 6337, Loss: 0.00014446766464971006\n",
      "Epoch 0, Batch 6436, Loss: 0.00013706812344025821\n",
      "Epoch 0, Batch 6535, Loss: 0.00013006896842853165\n",
      "Epoch 0, Batch 6634, Loss: 0.00012344648988801054\n",
      "Epoch 0, Batch 6733, Loss: 0.00011717857232724782\n",
      "Epoch 0, Batch 6832, Loss: 0.00011124450160423294\n",
      "Epoch 0, Batch 6931, Loss: 0.00010562494513578713\n",
      "Epoch 0, Batch 7030, Loss: 0.00010030199322500266\n",
      "Epoch 0, Batch 7129, Loss: 9.525868954369798e-05\n",
      "Epoch 0, Batch 7228, Loss: 9.04792553774314e-05\n",
      "Epoch 0, Batch 7327, Loss: 8.594888931838796e-05\n",
      "Epoch 0, Batch 7426, Loss: 8.165371196810157e-05\n",
      "Epoch 0, Batch 7525, Loss: 7.758074199955445e-05\n",
      "Epoch 0, Batch 7624, Loss: 7.371776162472088e-05\n",
      "Epoch 0, Batch 7723, Loss: 7.005324572673998e-05\n",
      "Epoch 0, Batch 7822, Loss: 6.657637241005431e-05\n",
      "Epoch 0, Batch 7921, Loss: 6.327700735710096e-05\n",
      "Epoch 0, Batch 8020, Loss: 6.014586790115572e-05\n",
      "Epoch 0, Batch 8119, Loss: 5.717366722819861e-05\n",
      "Epoch 1 training completed, Avg Loss: 0.0001\n",
      "Latest Model Saved1 with validation RMSE inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1/10 [03:39<32:53, 219.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation RMSE: 0.0001\n",
      "Model saved at epoch 1 with validation RMSE 0.1131\n",
      "Epoch 1, Batch 1, Loss: 5.554382369155065e-07\n",
      "Epoch 1, Batch 100, Loss: 5.3607349618687296e-05\n",
      "Epoch 1, Batch 199, Loss: 5.0966025264642666e-05\n",
      "Epoch 1, Batch 298, Loss: 4.8457377088197975e-05\n",
      "Epoch 1, Batch 397, Loss: 4.6075235986791084e-05\n",
      "Epoch 1, Batch 496, Loss: 4.381217226182344e-05\n",
      "Epoch 1, Batch 595, Loss: 4.1662848234409465e-05\n",
      "Epoch 1, Batch 694, Loss: 3.962047445384087e-05\n",
      "Epoch 1, Batch 793, Loss: 3.768037069676211e-05\n",
      "Epoch 1, Batch 892, Loss: 3.583659410651308e-05\n",
      "Epoch 1, Batch 991, Loss: 3.408459106140072e-05\n",
      "Epoch 1, Batch 1090, Loss: 3.2419507297163365e-05\n",
      "Epoch 1, Batch 1189, Loss: 3.08369290723931e-05\n",
      "Epoch 1, Batch 1288, Loss: 2.933271853180486e-05\n",
      "Epoch 1, Batch 1387, Loss: 2.7902787314815215e-05\n",
      "Epoch 1, Batch 1486, Loss: 2.6543442527326987e-05\n",
      "Epoch 1, Batch 1585, Loss: 2.52510830614483e-05\n",
      "Epoch 1, Batch 1684, Loss: 2.402246807832853e-05\n",
      "Epoch 1, Batch 1783, Loss: 2.2854230574012037e-05\n",
      "Epoch 1, Batch 1882, Loss: 2.1743328652519267e-05\n",
      "Epoch 1, Batch 1981, Loss: 2.068687324936036e-05\n",
      "Epoch 1, Batch 2080, Loss: 1.9682279653352452e-05\n",
      "Epoch 1, Batch 2179, Loss: 1.872715592980967e-05\n",
      "Epoch 1, Batch 2278, Loss: 1.7818670876295073e-05\n",
      "Epoch 1, Batch 2377, Loss: 1.6954478360275972e-05\n",
      "Epoch 1, Batch 2476, Loss: 1.6132415803440382e-05\n",
      "Epoch 1, Batch 2575, Loss: 1.53504076115496e-05\n",
      "Epoch 1, Batch 2674, Loss: 1.4606860486310324e-05\n",
      "Epoch 1, Batch 2773, Loss: 1.3899653422413394e-05\n",
      "Epoch 1, Batch 2872, Loss: 1.3226919672888471e-05\n",
      "Epoch 1, Batch 2971, Loss: 1.2586806988110766e-05\n",
      "Epoch 1, Batch 3070, Loss: 1.1977793583355379e-05\n",
      "Epoch 1, Batch 3169, Loss: 1.1398407814340316e-05\n",
      "Epoch 1, Batch 3268, Loss: 1.0847190460481214e-05\n",
      "Epoch 1, Batch 3367, Loss: 1.032272413794999e-05\n",
      "Epoch 1, Batch 3466, Loss: 9.823630325627164e-06\n",
      "Epoch 1, Batch 3565, Loss: 9.348829844384453e-06\n",
      "Epoch 1, Batch 3664, Loss: 8.897143979993416e-06\n",
      "Epoch 1, Batch 3763, Loss: 8.467405250485171e-06\n",
      "Epoch 1, Batch 3862, Loss: 8.058582297962857e-06\n",
      "Epoch 1, Batch 3961, Loss: 7.66958589792921e-06\n",
      "Epoch 1, Batch 4060, Loss: 7.299380622498575e-06\n",
      "Epoch 1, Batch 4159, Loss: 6.9471286678890465e-06\n",
      "Epoch 1, Batch 4258, Loss: 6.6119389066443545e-06\n",
      "Epoch 1, Batch 4357, Loss: 6.292928587754432e-06\n",
      "Epoch 1, Batch 4456, Loss: 5.989351548123523e-06\n",
      "Epoch 1, Batch 4555, Loss: 5.700488363800105e-06\n",
      "Epoch 1, Batch 4654, Loss: 5.425582198768097e-06\n",
      "Epoch 1, Batch 4753, Loss: 5.1639311868711955e-06\n",
      "Epoch 1, Batch 4852, Loss: 4.914905357509269e-06\n",
      "Epoch 1, Batch 4951, Loss: 4.677927854572772e-06\n",
      "Epoch 1, Batch 5050, Loss: 4.452388361642079e-06\n",
      "Epoch 1, Batch 5149, Loss: 4.237724415361299e-06\n",
      "Epoch 1, Batch 5248, Loss: 4.033414261357393e-06\n",
      "Epoch 1, Batch 5347, Loss: 3.838987277049455e-06\n",
      "Epoch 1, Batch 5446, Loss: 3.6539398411150612e-06\n",
      "Epoch 1, Batch 5545, Loss: 3.4778135659507826e-06\n",
      "Epoch 1, Batch 5644, Loss: 3.310176912236784e-06\n",
      "Epoch 1, Batch 5743, Loss: 3.1506226241617696e-06\n",
      "Epoch 1, Batch 5842, Loss: 2.9987861262270597e-06\n",
      "Epoch 1, Batch 5941, Loss: 2.85432290411336e-06\n",
      "Epoch 1, Batch 6040, Loss: 2.71683813934942e-06\n",
      "Epoch 1, Batch 6139, Loss: 2.585974405064917e-06\n",
      "Epoch 1, Batch 6238, Loss: 2.461414824210806e-06\n",
      "Epoch 1, Batch 6337, Loss: 2.3428556937687973e-06\n",
      "Epoch 1, Batch 6436, Loss: 2.230026716460998e-06\n",
      "Epoch 1, Batch 6535, Loss: 2.122637615684653e-06\n",
      "Epoch 1, Batch 6634, Loss: 2.0204203906359907e-06\n",
      "Epoch 1, Batch 6733, Loss: 1.9231360658977792e-06\n",
      "Epoch 1, Batch 6832, Loss: 1.8305622904790652e-06\n",
      "Epoch 1, Batch 6931, Loss: 1.742443367902524e-06\n",
      "Epoch 1, Batch 7030, Loss: 1.6585658954682004e-06\n",
      "Epoch 1, Batch 7129, Loss: 1.5787275538059475e-06\n",
      "Epoch 1, Batch 7228, Loss: 1.5027340612050466e-06\n",
      "Epoch 1, Batch 7327, Loss: 1.4303995362752176e-06\n",
      "Epoch 1, Batch 7426, Loss: 1.3615485067930422e-06\n",
      "Epoch 1, Batch 7525, Loss: 1.2960127025962719e-06\n",
      "Epoch 1, Batch 7624, Loss: 1.23363218222039e-06\n",
      "Epoch 1, Batch 7723, Loss: 1.174254963416388e-06\n",
      "Epoch 1, Batch 7822, Loss: 1.117745520105018e-06\n",
      "Epoch 1, Batch 7921, Loss: 1.0639591812378057e-06\n",
      "Epoch 1, Batch 8020, Loss: 1.0127605696652608e-06\n",
      "Epoch 1, Batch 8119, Loss: 9.640258576837367e-07\n",
      "Epoch 2 training completed, Avg Loss: 0.0000\n",
      "Latest Model Saved2 with validation RMSE 0.1131\n"
     ]
    }
   ],
   "source": [
    "model = AttentionModel()\n",
    "criterion = RMSLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"./trained_models/train-runs\")\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    # Train for one epoch\n",
    "    avg_train_loss = train_one_epoch(epoch, writer, model, criterion, optimizer)\n",
    "    print(f'Epoch {epoch+1} training completed, Avg Loss: {avg_train_loss:.4f}')\n",
    "    torch.save(model.state_dict(), './trained_models/latest_Att-CNN-LSTM_model.pt')\n",
    "    print(f'Latest Model Saved{epoch+1}')\n",
    "\n",
    "    # Validate after each epoch\n",
    "    avg_val_loss = validate_one_epoch(epoch, writer, model, criterion)\n",
    "\n",
    "    # Save the model if it has the best validation loss so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), './trained_models/best_Att-CNN-LSTM_model.pt')\n",
    "        print(f'Model saved at epoch {epoch+1} with validation RMSE {best_val_loss:.4f}')\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(['ID_Zindi', 'ID'], axis=1)\n",
    "test = test.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = NO2Model(embedding_dim=16, hidden_dim=64, num_layers=2, fc_out_dim=32)\n",
    "model.load_state_dict(torch.load('best_NO2_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NO2Test(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.data = df.sort_values(by=['LAT', 'LON', 'Date']).reset_index(drop=True)\n",
    "        self.locations = self.data.groupby(['LAT', 'LON']).groups\n",
    "        self.location_keys = list(self.locations.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.location_keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        location = self.location_keys[idx]\n",
    "        indices = self.locations[location]\n",
    "        \n",
    "        # Extract each feature as a separate tensor\n",
    "        lst_seq = torch.tensor(self.data.loc[indices, 'LST'].values, dtype=torch.float32).unsqueeze(1)\n",
    "        aai_seq = torch.tensor(self.data.loc[indices, 'AAI'].values, dtype=torch.float32).unsqueeze(1)\n",
    "        cloud_fraction_seq = torch.tensor(self.data.loc[indices, 'CloudFraction'].values, dtype=torch.float32).unsqueeze(1)\n",
    "        precipitation_seq = torch.tensor(self.data.loc[indices, 'Precipitation'].values, dtype=torch.float32).unsqueeze(1)\n",
    "        tropopause_pressure_seq = torch.tensor(self.data.loc[indices, 'TropopausePressure'].values, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        # Extract LAT and LON\n",
    "        lat = torch.tensor(location[0], dtype=torch.float32)\n",
    "        lon = torch.tensor(location[1], dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        return lst_seq, aai_seq, cloud_fraction_seq, precipitation_seq, tropopause_pressure_seq, lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset and DataLoader\n",
    "test_dataset = NO2Test(test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the test data\n",
    "with torch.no_grad():\n",
    "    for idx, (lst_seq, aai_seq, cloud_fraction_seq, precipitation_seq, tropopause_pressure_seq, lat, lon) in enumerate(test_loader):\n",
    "        # Forward pass\n",
    "        output = model(lst_seq, aai_seq, cloud_fraction_seq, precipitation_seq, tropopause_pressure_seq, lat, lon)\n",
    "        \n",
    "        # Store the prediction (converting output to a Python float)\n",
    "        test.loc[test_dataset.locations[test_dataset.location_keys[idx]], 'Predicted_NO2'] = output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(by=['LAT', 'LON', 'Date']).reset_index(drop=True)\n",
    "test.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4141],\n",
      "        [-0.4849],\n",
      "        [-0.5953],\n",
      "        [-0.4074],\n",
      "        [-0.6392],\n",
      "        [-0.5918],\n",
      "        [-0.6847],\n",
      "        [-0.4437],\n",
      "        [-0.5906],\n",
      "        [-0.4310],\n",
      "        [-0.5093],\n",
      "        [-0.4230],\n",
      "        [-0.5049],\n",
      "        [-0.4352],\n",
      "        [-0.4845],\n",
      "        [-0.4977],\n",
      "        [-0.7832],\n",
      "        [-0.5389],\n",
      "        [-0.3613],\n",
      "        [-0.5073],\n",
      "        [-0.4512],\n",
      "        [-0.2741],\n",
      "        [-0.6021],\n",
      "        [-0.3383],\n",
      "        [-0.6961],\n",
      "        [-0.4471],\n",
      "        [-0.5489],\n",
      "        [-0.7116],\n",
      "        [-0.2524],\n",
      "        [-0.2971],\n",
      "        [-0.4675],\n",
      "        [-0.5110],\n",
      "        [-0.6584],\n",
      "        [-0.5725],\n",
      "        [-0.5204],\n",
      "        [-0.4848],\n",
      "        [-0.4254],\n",
      "        [-0.1992],\n",
      "        [-0.4102],\n",
      "        [-0.4971],\n",
      "        [-0.2304],\n",
      "        [-0.3034],\n",
      "        [-0.3974],\n",
      "        [-0.6545],\n",
      "        [-0.4609],\n",
      "        [-0.3045],\n",
      "        [-0.6572],\n",
      "        [-0.3383],\n",
      "        [-0.3107],\n",
      "        [-0.5964],\n",
      "        [-0.7177],\n",
      "        [-0.6477],\n",
      "        [-0.6553],\n",
      "        [-0.6610],\n",
      "        [-0.6035],\n",
      "        [-0.4840],\n",
      "        [-0.4774],\n",
      "        [-0.2365],\n",
      "        [-0.6114],\n",
      "        [-0.3765],\n",
      "        [-0.5455],\n",
      "        [-0.4853],\n",
      "        [-0.4146],\n",
      "        [-0.5455],\n",
      "        [-0.5295],\n",
      "        [-0.5451],\n",
      "        [-0.5466],\n",
      "        [-0.2503],\n",
      "        [-0.4552],\n",
      "        [-0.6156],\n",
      "        [-0.6994],\n",
      "        [-0.5702],\n",
      "        [-0.4305],\n",
      "        [-0.3872],\n",
      "        [-0.3777],\n",
      "        [-0.6049],\n",
      "        [-0.2629],\n",
      "        [-0.4431],\n",
      "        [-0.2103],\n",
      "        [-0.5111],\n",
      "        [-0.4476],\n",
      "        [-0.4154],\n",
      "        [-0.0877],\n",
      "        [-0.3791],\n",
      "        [-0.4591],\n",
      "        [-0.5559],\n",
      "        [-0.5063],\n",
      "        [-0.6054],\n",
      "        [-0.5957],\n",
      "        [-0.3579],\n",
      "        [-0.4360],\n",
      "        [-0.2734],\n",
      "        [-0.3895],\n",
      "        [-0.3141],\n",
      "        [-0.5713],\n",
      "        [-0.3945],\n",
      "        [-0.3089],\n",
      "        [-0.5742],\n",
      "        [-0.6505],\n",
      "        [-0.4096],\n",
      "        [-0.5172],\n",
      "        [-0.4633],\n",
      "        [-0.5585],\n",
      "        [-0.4872],\n",
      "        [-0.3857],\n",
      "        [-0.5865],\n",
      "        [-0.4697],\n",
      "        [-0.4365],\n",
      "        [-0.3803],\n",
      "        [-0.5915],\n",
      "        [-0.3817],\n",
      "        [-0.5139],\n",
      "        [-0.2592],\n",
      "        [-0.3228],\n",
      "        [-0.4892],\n",
      "        [-0.4459],\n",
      "        [-0.3082],\n",
      "        [-0.6612],\n",
      "        [-0.7196],\n",
      "        [-0.4669],\n",
      "        [-0.5148],\n",
      "        [-0.5070],\n",
      "        [-0.6138],\n",
      "        [-0.5711],\n",
      "        [-0.6520],\n",
      "        [-0.6154],\n",
      "        [-0.3577],\n",
      "        [-0.3826]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = 128\n",
    "\n",
    "features = torch.randn(batch, 15, 8)\n",
    "lat = torch.randn(batch, 15, 1)\n",
    "lon = torch.randn(batch, 15, 1)\n",
    "\n",
    "output = model(features,lat,lon)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
