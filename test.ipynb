{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from model import AttentionModel, XGBoostModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NO2Dataset(Dataset):\n",
    "    def __init__(self, df, max_days=15):\n",
    "        self.max_days = max_days\n",
    "        # Keep a global index directly from the DataFrame's index after sorting\n",
    "        self.data = df.sort_values(by=['LAT', 'LON', 'Date']).reset_index(drop=False)\n",
    "        self.data['Date'] = pd.to_datetime(self.data['Date'])\n",
    "        \n",
    "        # Create a column for global indices using the reset index\n",
    "        self.data['global_idx'] = self.data.index\n",
    "        \n",
    "        self.locations = self.data.groupby(['LAT', 'LON']).groups\n",
    "        self.location_keys = list(self.locations.keys())\n",
    "\n",
    "         # Prepare samples with (location, global index, id_zindi)\n",
    "        self.samples = []\n",
    "        for loc in self.location_keys:\n",
    "            for idx in self.locations[loc]:\n",
    "                id_zindi = self.data.loc[idx, 'ID_Zindi']  # Assuming this column exists\n",
    "                self.samples.append((loc, idx, id_zindi))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the location and index from self.samples (global index)\n",
    "        location, global_data_idx, id_string = self.samples[idx]\n",
    "\n",
    "        # Get all data for the location and sort by 'Date'\n",
    "        loc_data = self.data.loc[self.locations[location]].sort_values(by='Date')\n",
    "\n",
    "        # Instead of filtering by global_idx again, directly access the row using iloc\n",
    "        loc_data_row = loc_data.iloc[(global_data_idx - loc_data.index[0])]\n",
    "\n",
    "        if loc_data_row is None:\n",
    "            print(f\"No data found for global index {global_data_idx} in location {location}\")\n",
    "            return None  # Handle this case appropriately\n",
    "        \n",
    "        # Extract the current date from the location data\n",
    "        current_date = loc_data_row['Date']\n",
    "        \n",
    "        # Define date range for the last `max_days` days including current date\n",
    "        start_date = current_date - pd.DateOffset(days=self.max_days - 1)\n",
    "        end_date = current_date\n",
    "\n",
    "        # Get past data for the last `max_days` days\n",
    "        past_data = loc_data[(loc_data['Date'] >= start_date) & (loc_data['Date'] <= end_date)]\n",
    "\n",
    "        # Padding if fewer than `max_days` days\n",
    "        if len(past_data) < self.max_days:\n",
    "            num_padding_days = self.max_days - len(past_data)\n",
    "            padding_dates = pd.date_range(end=start_date - pd.DateOffset(days=1), periods=num_padding_days)\n",
    "            padding_data = pd.DataFrame({\n",
    "                'Date': padding_dates,\n",
    "                'LAT': loc_data['LAT'].iloc[0],  # Fill with location LAT\n",
    "                'LON': loc_data['LON'].iloc[0],  # Fill with location LON\n",
    "                'LST': 0, 'AAI': 0, 'CloudFraction': 0, 'Precipitation': 0, 'NO2_strat': 0, \n",
    "                'NO2_total': 0, 'NO2_trop': 0, 'TropopausePressure': 0,\n",
    "                'index': -1,  # Use -1 to indicate padding rows\n",
    "                'global_idx': -1  # Same for global_idx\n",
    "                })\n",
    "\n",
    "            # Concatenate padding and past data\n",
    "            past_data = pd.concat([padding_data, past_data], ignore_index=True)\n",
    "\n",
    "        # Sort past data again (optional) to ensure order\n",
    "        past_data = past_data.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "        # Extract the relevant features and convert to tensors\n",
    "        features_tensor = torch.tensor(past_data[['LST', 'AAI', 'CloudFraction', 'Precipitation', \n",
    "                                                  'NO2_strat', 'NO2_total', 'NO2_trop', \n",
    "                                                  'TropopausePressure', 'LAT', 'LON']].values, dtype=torch.float32)\n",
    "        # lat = torch.tensor(past_data['LAT'].values[0], dtype=torch.float32)  # Only one LAT value\n",
    "        # lon = torch.tensor(past_data['LON'].values[0], dtype=torch.float32)  # Only one LON value\n",
    "        \n",
    "        # Return the feature tensor, lat/lon, and ground truth\n",
    "        return features_tensor, id_string\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features, id_string = zip(*batch)\n",
    "\n",
    "    features_padded = pad_sequence(features, batch_first=True)  # (batch_size, max_seq_len, num_features)\n",
    "\n",
    "\n",
    "    return features_padded, id_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, xgb, test_loader):\n",
    "    model.eval() \n",
    "    model.to(device)  \n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for i, (features_seq, ids) in enumerate(test_loader):\n",
    "            features_seq = features_seq.to(device)\n",
    "            test_outputs = model(features_seq)\n",
    "            predicitions = xgb.inference(test_outputs)\n",
    "            # print(\"------------------------------\")\n",
    "            # print(test_outputs.squeeze().cpu().numpy())\n",
    "            # print(\"------------------------------\")\n",
    "            all_preds.append(predicitions.cpu().numpy())\n",
    "            all_ids.extend(ids) \n",
    "\n",
    "            print(f'Batch {i+1} processed', end='\\r')\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    print(f'Number of predictions: {len(all_preds)}')\n",
    "    print(f'Number of IDs: {len(all_ids)}')\n",
    "\n",
    "    if len(all_preds) != len(all_ids):\n",
    "        print(\"Warning: The number of predictions and IDs do not match!\")\n",
    "    \n",
    "    results_df = pd.DataFrame({'ID': all_ids, 'Predicted_NO2': all_preds})\n",
    "    results_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionModel(\n",
       "  (conv1d): Conv1d(10, 64, kernel_size=(1,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bilstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
       "  (attention_block): AttentionBlock(\n",
       "    (multihead_attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=15, bias=True)\n",
       "    (attention_fc): Linear(in_features=15, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
       "  (bn2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = 'trained-model-xgboost/best_Att-CNN-LSTM_model_22.pt'\n",
    "\n",
    "model = AttentionModel()\n",
    "xgb = XGBoostModel(0)\n",
    "state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./Test_Cleaned_KNN.csv')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "no2_dataset = NO2Dataset(dataset)\n",
    "test_loader = DataLoader(no2_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 6576\n",
      "Number of IDs: 6576\n"
     ]
    }
   ],
   "source": [
    "test_model(model, xgb, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
